{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UIPr3uHXlWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "aed66fe4-2937-4a69-f9af-ac1b0fd7e932"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import math\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import cv2\n",
        "import skimage\n",
        "\n",
        "import tensorflow.python.platform\n",
        "from scipy.misc import imread, imresize\n",
        "from keras.preprocessing import sequence\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7fTE2K8XyQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "cb213819-b453-4f9c-b363-89628747d4da"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 124kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.2)\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCgiwQEX50q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "52b65db9-ee5c-4364-8858-d019787c6687"
      },
      "source": [
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 246kB/s \n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXlCaitZOe6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c65e413-a6cb-4038-d547-fa6c5592c95f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOPfma2OOujH"
      },
      "source": [
        "model_path = 'drive/My Drive/TDLProj/FINAL_200'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OduvW-V4YR49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47fc298f-0b83-4438-8386-5e4ff8c5d5f5"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "from keras import Model\n",
        "\n",
        "model = VGG16()\n",
        "model.layers.pop()\n",
        "#model.layers.pop()\n",
        "model=Model(inputs=model.inputs,outputs=model.layers[-1].output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ilVYx5YYUX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c9fb49de-b7ac-426a-9fbf-be34458ffb6f"
      },
      "source": [
        "img_path = 'drive/My Drive/TDLProj/IMG-20200229-WA0003.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img_data = image.img_to_array(img)\n",
        "img_data = np.expand_dims(img_data, axis=0)\n",
        "img_data = preprocess_input(img_data)\n",
        "vgg16_feature = model.predict(img_data)\n",
        "feat=vgg16_feature\n",
        "print(feat.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "(1, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSDWAyidYgj8"
      },
      "source": [
        "dim_embed = 256\n",
        "dim_hidden = 256\n",
        "dim_in = 4096\n",
        "batch_size = 1\n",
        "learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "n_epochs = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqm2WXPBZXBr"
      },
      "source": [
        "class Caption_Generator():\n",
        "    def __init__(self, dim_in, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, init_b=None):\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_embed = dim_embed\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.batch_size = batch_size\n",
        "        self.n_lstm_steps = n_lstm_steps\n",
        "        self.n_words = n_words\n",
        "\n",
        "        # declare the variables to be used for our word embeddings\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            self.word_embedding = tf.Variable(tf.random_uniform([self.n_words, self.dim_embed], -0.1, 0.1), name='word_embedding')\n",
        "\n",
        "        self.embedding_bias = tf.Variable(tf.zeros([dim_embed]), name='embedding_bias')\n",
        "\n",
        "        # declare the LSTM itself\n",
        "        self.lstm = tf.contrib.rnn.BasicLSTMCell(dim_hidden)\n",
        "\n",
        "        # declare the variables to be used to embed the image feature embedding to the word embedding space\n",
        "        self.img_embedding = tf.Variable(tf.random_uniform([dim_in, dim_hidden], -0.1, 0.1), name='img_embedding')\n",
        "        self.img_embedding_bias = tf.Variable(tf.zeros([dim_hidden]), name='img_embedding_bias')\n",
        "\n",
        "        # declare the variables to go from an LSTM output to a word encoding output\n",
        "        self.word_encoding = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='word_encoding')\n",
        "\n",
        "        # optional initialization setter for encoding bias variable\n",
        "        if init_b is not None:\n",
        "            self.word_encoding_bias = tf.Variable(init_b, name='word_encoding_bias')\n",
        "        else:\n",
        "            self.word_encoding_bias = tf.Variable(tf.zeros([n_words]), name='word_encoding_bias')\n",
        "\n",
        "    def build_model(self):\n",
        "        # declaring the placeholders for our extracted image feature vectors, our caption, and our mask\n",
        "        # (describes how long our caption is with an array of 0/1 values of length `maxlen`\n",
        "        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])\n",
        "        caption_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
        "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
        "\n",
        "        # getting an initial LSTM embedding from our image_imbedding\n",
        "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
        "\n",
        "        # setting initial state of our LSTM\n",
        "        state = self.lstm.zero_state(self.batch_size, dtype=tf.float32)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        with tf.variable_scope(\"RNN\"):\n",
        "            for i in range(self.n_lstm_steps):\n",
        "                if i > 0:\n",
        "                   # if this isn’t the first iteration of our LSTM we need to get the word_embedding corresponding\n",
        "                   # to the (i-1)th word in our caption\n",
        "                    with tf.device(\"/cpu:0\"):\n",
        "                        current_embedding = tf.nn.embedding_lookup(self.word_embedding, caption_placeholder[:,i-1]) + self.embedding_bias\n",
        "                else:\n",
        "                     #if this is the first iteration of our LSTM we utilize the embedded image as our input\n",
        "                    current_embedding = image_embedding\n",
        "                if i > 0:\n",
        "                    # allows us to reuse the LSTM tensor variable on each iteration\n",
        "                    tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "                out, state = self.lstm(current_embedding, state)\n",
        "\n",
        "\n",
        "                if i > 0:\n",
        "                    #get the one-hot representation of the next word in our caption\n",
        "                    labels = tf.expand_dims(caption_placeholder[:, i], 1)\n",
        "                    ix_range=tf.range(0, self.batch_size, 1)\n",
        "                    ixs = tf.expand_dims(ix_range, 1)\n",
        "                    concat = tf.concat([ixs, labels],1)\n",
        "                    onehot = tf.sparse_to_dense(\n",
        "                            concat, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
        "\n",
        "\n",
        "                    #perform a softmax classification to generate the next word in the caption\n",
        "                    logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
        "                    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=onehot)\n",
        "                    xentropy = xentropy * mask[:,i]\n",
        "\n",
        "                    loss = tf.reduce_sum(xentropy)\n",
        "                    total_loss += loss\n",
        "\n",
        "            total_loss = total_loss / tf.reduce_sum(mask[:,1:])\n",
        "            return total_loss, img,  caption_placeholder, mask\n",
        "\n",
        "\n",
        "    def build_generator(self, maxlen, batchsize=1):\n",
        "        #same setup as `build_model` function\n",
        "        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])\n",
        "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
        "        state = self.lstm.zero_state(batchsize,dtype=tf.float32)\n",
        "\n",
        "        #declare list to hold the words of our generated captions\n",
        "        all_words = []\n",
        "        with tf.variable_scope(\"RNN\"):\n",
        "            # in the first iteration we have no previous word, so we directly pass in the image embedding\n",
        "            # and set the `previous_word` to the embedding of the start token ([0]) for the future iterations\n",
        "            output, state = self.lstm(image_embedding, state)\n",
        "            previous_word = tf.nn.embedding_lookup(self.word_embedding, [0]) + self.embedding_bias\n",
        "\n",
        "            for i in range(maxlen):\n",
        "                tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "                out, state = self.lstm(previous_word, state)\n",
        "\n",
        "\n",
        "                # get a get maximum probability word and it's encoding from the output of the LSTM\n",
        "                logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
        "                best_word = tf.argmax(logit, 1)\n",
        "\n",
        "                with tf.device(\"/cpu:0\"):\n",
        "                    # get the embedding of the best_word to use as input to the next iteration of our LSTM\n",
        "                    previous_word = tf.nn.embedding_lookup(self.word_embedding, best_word)\n",
        "\n",
        "                previous_word += self.embedding_bias\n",
        "\n",
        "                all_words.append(best_word)\n",
        "\n",
        "        return img, all_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F72C4pE1ZcWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "39ae4f9d-eefa-450a-c692-f8eb219b18d5"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "'''with open(vgg_path,'rb') as f:\n",
        "    fileContent = f.read()\n",
        "    graph_def = tf.GraphDef()\n",
        "    graph_def.ParseFromString(fileContent)\n",
        "\n",
        "images = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
        "tf.import_graph_def(graph_def, input_map={\"images\":images})'''\n",
        "\n",
        "ixtoword = np.load('drive/My Drive/TDLProj/ixtoword.npy').tolist()\n",
        "n_words = len(ixtoword)\n",
        "maxlen=15\n",
        "graph = tf.get_default_graph()\n",
        "sess = tf.InteractiveSession(graph=graph)\n",
        "caption_generator = Caption_Generator(dim_in, dim_hidden, dim_embed, batch_size, maxlen+2, n_words)\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "imagee, generated_words = caption_generator.build_generator(maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-7-7c33881bbeac>:18: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwU42ju8Zgxi"
      },
      "source": [
        "def test(sess,image,generated_words,ixtoword,feat): # Naive greedy search\n",
        "\n",
        "\n",
        "\n",
        "    #feat = read_image(test_image_path)\n",
        "    #fc7 = sess.run(graph.get_tensor_by_name(\"import/Relu_1:0\"), feed_dict={images:feat})\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    sanity_check=False\n",
        "    # sanity_check=True\n",
        "    if not sanity_check:\n",
        "        saved_path=tf.train.latest_checkpoint(model_path)\n",
        "        saver.restore(sess, saved_path)\n",
        "    else:\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "    generated_word_index= sess.run(generated_words, feed_dict={image:feat})\n",
        "    generated_word_index = np.hstack(generated_word_index)\n",
        "    generated_words = [ixtoword[x] for x in generated_word_index]\n",
        "    punctuation = np.argmax(np.array(generated_words) == '.')+1\n",
        "\n",
        "    generated_words = generated_words[:punctuation]\n",
        "    generated_sentence = ' '.join(generated_words)\n",
        "    print(generated_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qiC_kyJZvG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "82d5fb5b-fb31-41e7-97e5-4a05297f80de"
      },
      "source": [
        "test(sess,imagee,generated_words,ixtoword,feat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/TDLProj/FINAL_200/model-152\n",
            "two women and a man are smiling and walking in a park .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0mBf0xOaIUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b90b496-fe3e-458c-9699-9cb4c12eacc8"
      },
      "source": [
        "countzero_in1 = np.count_nonzero(feat)\n",
        "print(countzero_in1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXbgLGyyqu8S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}